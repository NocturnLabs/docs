# Deep Reasoning Suite

The **Deep Reasoning Suite** is a dual-component intelligence system designed for high-complexity cognitive tasks. It unifies immediate, interactive reasoning via a terminal-based UI with long-form, multi-source deep research capabilities powered by Google Gemini.

This suite consists of two primary tools:
1.  **AI Chat TUI**: A beautiful, terminal-native interface for reasoning models (IO Intelligence, Perplexity).
2.  **Deep Research CLI**: An autonomous research agent capable of recursive web searching, synthesis, and session management.

> [!NOTE]
> These tools share a philosophical foundation of "Thinking Process" visibility but operate as distinct executables optimized for their respective workflows.

---

## 1. AI Chat TUI

A "Minimal, Beautiful, Fast" terminal user interface (TUI) built in Python using `Textual` and `Rich`. It provides a distraction-free environment for interacting with reasoning-heavy LLMs.

### Installation

**Prerequisites**: Python 3.10+

```bash
# Navigate to the project root
cd deep-reasoning

# Install dependencies (recommended to use a virtual environment)
pip install openai rich
```

### Configuration

The TUI relies on environment variables for authentication.

| Variable | Description | Required |
| :--- | :--- | :--- |
| `IO_INTELLIGENCE_API_KEY` | Key for IO Intelligence API (DeepSeek models) | Yes (if using IO) |
| `PERPLEXITY_API_KEY` | Key for Perplexity API (Sonar Reasoning models) | Yes (if using Pplx) |

### Usage

Run the application directly:

```bash
python3 ai_chat.py
```

#### Interactive Controls

Once inside the TUI, the following commands are available during the chat session:

-   **/model**: Switch between available models (e.g., `deepseek-ai/DeepSeek-R1`, `sonar-reasoning-pro`).
-   **/clear**: Clear the current conversation history and screen.
-   **/quit** (or **exit**): Terminate the application.

#### Batch Processing Mode

For high-throughput automated reasoning, the TUI supports a "Batch Mode" that reads from a file.

**Input File Format**: `questions.txt`
The file must separate distinct questions using the delimiter `[(END)]`.

```text
Explain quantum entanglement in simple terms.
[(END)]
Compare Rust vs C++ for system programming.
[(END)]
```

**Execution**:
1.  Ensure `questions.txt` exists in the working directory.
2.  Run `python3 ai_chat.py`.
3.  Select "Yes" when prompted for "Enable multiple questions mode?" (or run in a non-interactive shell).

> [!IMPORTANT]
> Batch mode implements **concurrency limits** (default: 10 concurrent requests) and **periodic saving** (every 20 items) to SQLite to prevent data loss during long runs.

### Data Persistence

All conversations are automatically logged to a local SQLite database.

**File**: `MAIN_chat_history.db`
**Schema**:
```sql
CREATE TABLE chat_history (
    id INTEGER PRIMARY KEY,
    timestamp TEXT NOT NULL,
    provider TEXT NOT NULL,
    model TEXT NOT NULL,
    user_question TEXT NOT NULL,
    assistant_message TEXT NOT NULL,
    prompt_tokens INTEGER,
    completion_tokens INTEGER,
    total_tokens INTEGER
);
```

---

## 2. Deep Research CLI

A Node.js-based autonomous agent that leverages Google's Gemini Deep Research API. It goes beyond simple Q&A by performing multi-step web research, synthesizing sources, and maintaining complex session states.

### Installation

**Prerequisites**: Node.js 18+

```bash
cd deep-reasoning/deep-research

# Install dependencies
npm install

# Build the project
npm run build
```

**Quick Setup**:
Run the initialization script to set up defaults:
```bash
./init.sh
```

### Configuration

Create a `config.yaml` or `config.json` in the `config/` directory.

**Example `config.yaml`**:
```yaml
api:
  apiKey: "YOUR_GEMINI_KEY" # Optional if env var is set
  model: "gemini-2.0-flash-thinking-exp-1219"
  temperature: 0.7
  maxOutputTokens: 8192
  searchEnabled: true  # Allow external web queries

logging:
  directory: "./logs"
  format: "json"       # Options: json, csv
```

### Core Commands

The CLI is invoked via `npm run start -- [COMMAND]`.

#### 1. Research (Standard Mode)
Perform a deep dive on a single topic.

```bash
# Simple query
npm run start -- research "Impact of autonomous coding agents on software jobs"

# Interactive mode (allows follow-up questions)
npm run start -- research "History of the Roman Senate" --interactive
```

**Options**:
-   `--format`: Output format (`text` (default), `json`, `markdown`).
-   `--session-name`: Attach to a named persistent session.
-   `--plugin`: Path to a custom output formatter.

#### 2. Batch Processor
Process a queue of research topics from a file.

```bash
npm run start -- batch queries.txt --delay 2000 --resume
```

> [!TIP]
> The `--resume` flag is critical for long running jobs. It checks for a `.progress.json` file and skips already completed queries.

#### 3. Analytics Dashboard
View usage costs and token statistics.

```bash
npm run start -- analytics
```

**Example Output**:
```text
=== Analytics Dashboard ===

Usage Statistics:
  Total Queries: 142
  Total Tokens: 4,251,000
  Error Rate: 1.40%

Token Analytics:
  Average Tokens per Query: 29,936
  Token Distribution:
    Large (> 5000): 115

Cost Breakdown:
  Total Cost: $4.25 USD
  Cost per Query: $0.0299 USD
```

#### 4. Session Management
Manage long-running research contexts.

-   `list`: View all active sessions.
-   `compare <id1> <id2>`: Analyze overlap and unique insights between two research sessions.
-   `export [file]`: Dump session data to JSON.

```bash
npm run start -- sessions compare "roman-history" "greek-history"
```

#### 5. API Server
Start a REST API server to expose deep research capabilities to other applications.

```bash
npm run start -- server --port 3001
```

**Endpoints**:
-   `POST /research`: Trigger a new research job.
-   `GET /research/:id`: Polling endpoint for results.
-   `GET /analytics/*`: Retrieve usage stats via HTTP.

### Technical Architecture

**Modular Service Design**:
The system is built on a service-oriented architecture:
-   **ResearchService**: Orchestrates the Gemini API and DuckDuckGo search tools.
-   **SessionManager**: Handles state persistence using JSON-based file stores.
-   **LoggerService**: Rotates and structures conversation logs for auditing.
-   **AnalyticsService**: Aggregates token usage and calculates costs based on current pricing models.

**Security**:
-   **Input Validation**: Strict regex sanitation blocks potential XSS/Command Injection patterns (e.g., `<script>`, `javascript:`).
-   **Rate Limiting**: Configurable delays in batch mode prevent API bans.
